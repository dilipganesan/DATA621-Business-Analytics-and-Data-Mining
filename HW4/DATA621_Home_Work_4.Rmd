---
title: "DATA621_Home_Work_4"
author: "Dilip Ganesan"
date: "7/5/2018"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require('corrplot')) install.packages('corrplot')
library(corrplot)
if (!require('knitr')) install.packages('knitr')
library(knitr)
if (!require('ResourceSelection')) install.packages('ResourceSelection')
library(ResourceSelection)
if (!require('caret')) (install.packages('caret'))
library(caret)
if (!require('e1071')) (install.packages('e1071'))
library(e1071)
if (!require('pROC')) (install.packages('pROC'))
library(pROC)
if (!require('psych')) (install.packages('psych'))
library(psych)
if (!require('dplyr')) (install.packages('dplyr'))
library(dplyr)
if (!require('VIM')) (install.packages('VIM'))
library(VIM)
if (!require('reshape')) (install.packages('reshape'))
library(reshape)
if (!require('MASS')) (install.packages('MASS'))
library(MASS)
if (!require('car')) (install.packages('car'))
library(car)
if (!require('DataExplorer')) (install.packages('DataExplorer'))
library(DataExplorer)


```

## Home Work Assignment 4.

##1.DATA EXPLORATION.(Exploratory Data Analysis EDA)

As first step in our EDA, let us load the train data and do a summary statistics on the loaded dataset.



```{r}
# Let us load the train.csv data
train = read.csv('insurance_training_data.csv')
test = read.csv('insurance-evaluation-data.csv')

train = within(train, rm('INDEX'))
test = within(test, rm('INDEX'))

summary(train)


```

We will make sure there is no inappropriate distribution of target(response) variables in our training data.

```{r}
knitr::kable(table(train$TARGET_FLAG))

```


## Examination of Data Set.

1. There are 8161 rows and 25 columns(excluding the INDEX) in the train data set. The 2 columns are response variable and rest are predictor variables.

TARGET_FLAG is a binary variable where 1 means that person had a crash. 0 means that person did not had the crash.

TARGET_AMT is the expense because of the accident. 0 when there is no accident.

## Statistical Summary of Data Set:

```{r}
summary = describe(train, quant = c(.25,.75))
knitr::kable(summary)
```


1. CAR_AGE, YOJ and AGE has NA values.

2. CAR_AGE min value is -3. This needs some manipulation.

3. TARGET_AMT has a large skewness. We need to do some transformation on this variable.



## Visual Exploration of Data set:

***Histogram***

```{r}
ggtrain = split_columns(train)

plot_histogram(ggtrain$continuous)
plot_bar(ggtrain$discrete)

```


1. From the histogram we can see that variable TARGET_AMT is skewed. This is a ideal candidate for transformation.

2. We can see TARGET_FLAG has more observations of not having accidents compared to having an accidents.


***ScatterPlot***

```{r}
plot_scatterplot(train[1:25,], "TARGET_AMT", position = "jitter")

```


On the analysis of Scatter plot between TARGET_AMT and other Predictor variables, we do not see any pronounced positive or negative relationship.


***MultiCollinearity between predictor variables and also with response variables***

```{r}
#TARGET_AMT,KIDSDRIV,AGE,YOJ,TRAVTIME,TIF,CLM_FREQ,MVR_PTS,CAR_AGE, INCOME,HOME_VAL,BLUEBOOK,OLDCLAIM



cordata = cor(ggtrain$continuous)
corrplot(cordata, method = "square", type = "upper")

```



From the corrplot we can see that KIDSDRIV and HOMEKIDS has a litte positive correlation. 

With respect to response and predictor variables, We do not see much of a bigger correlation.

***Missing Value***

On anlysis of missing values, we see CAR_AGE, YOJ and AGE has missing values in the respective order. 

```{r}

VIM::aggr(train, col=c('navyblue','yellow'),
                      numbers=TRUE, sortVars=TRUE,
                       labels=names(train), 
                       ylab=c("NA Counts","Pattern"))
```
## 2.DATA PREPARATION 

From our visual exploration we have identified few variables to go through transformation based on the dataset.

1. We will make Home Kids as a Boolean instead of Factor.

2. Will reset the -ve values of CAR_AGE to 0.

3. We will change the Jobs and Education Levels.

4. For the variables CAR_AGE, AGE, YOJ we fill those missing values with Median/Mean.

We tried executing imputing these random missing values using MICE package.

But running the package lead to crashing of R server multiple times. So for this project dropped from using mice.

```{r}
train$HOMEKIDS[train$HOMEKIDS != 0 ] = 1

train$CAR_AGE[train$CAR_AGE < 0 ] = 0

train$JOB = as.character(train$JOB)
train$JOB[train$JOB == ""] = "Miscellaneous"
train$JOB <- as.factor(train$JOB)

train$EDUCATION <- ifelse(train$EDUCATION %in% c("PhD", "Masters"), 0, 1)


# ## Trying to use Mice package to fill in the missing values****
# mice_train =  mice(train, m = 1, maxit = 1, print = FALSE)
# train <- complete(mice_train)
# 
# 
# ###################


m = mean(train$AGE, na.rm = T)
train$AGE[is.na(train$AGE)] <- m

m = median(train$CAR_AGE, na.rm = T)
train$CAR_AGE[is.na(train$CAR_AGE)] = m

m = mean(train$YOJ, na.rm = T)
train$YOJ[is.na(train$YOJ)] = m

train$INCOME = as.numeric(train$INCOME)
train$HOME_VAL= as.numeric(train$HOME_VAL)
train$BLUEBOOK = as.numeric(train$BLUEBOOK)
train$OLDCLAIM= as.numeric(train$OLDCLAIM)

############## Transformation of Test Data#####################
test$HOMEKIDS[test$HOMEKIDS != 0 ] = 1  
test$CAR_AGE[test$CAR_AGE < 0 ] = 0

test$JOB = as.character(test$JOB)
test$JOB[test$JOB == ""] = "Miscellaneous"
test$JOB <- as.factor(test$JOB)

test$EDUCATION <- ifelse(test$EDUCATION %in% c("PhD", "Masters"), 0, 1)

m = mean(test$AGE, na.rm = T)
test$AGE[is.na(test$AGE)] = m

m = median(test$CAR_AGE, na.rm = T)
test$CAR_AGE[is.na(test$CAR_AGE)] = m

m = mean(test$YOJ, na.rm = T)
test$YOJ[is.na(test$YOJ)] = m

test$INCOME = as.numeric(test$INCOME)
test$HOME_VAL= as.numeric(test$HOME_VAL)
test$BLUEBOOK = as.numeric(test$BLUEBOOK)
test$OLDCLAIM= as.numeric(test$OLDCLAIM)
```


We will plot and see the missing elements. This is after filling the missed values.

We are making sure that there are no missing values.

```{r}
plot_missing(train)

```


## 3. BUILDING MODELS

### Classification:

As approach we are going to build the following models. 

Each of our logistic regression models will use bionomial regression with a logit link function

***Base Model and Transformed Variables***

1. The first model will be Base Model. It will contain all transformed data. It contains all the 24 variables(excluding TARGET_AMT)



```{r}

base_transform_model = glm(TARGET_FLAG ~ . -TARGET_AMT , family = binomial(link = 'logit'), data = train)

summary(base_transform_model)

knitr::kable(vif(base_transform_model))

hoslem.test(train$TARGET_FLAG, fitted(base_transform_model))

rocBaseTransformPlot = roc(base_transform_model$y, fitted(base_transform_model))
AUC = as.numeric(pROC::roc(base_transform_model$y, fitted(base_transform_model))$auc)
AUC
```


***Observations***

From the above model, we see 8 out of the 25 variables has (stat-sig) p-values at a significance level greater than 0.05
These variable can be dropped in our next model to see how our model performs. The below are the variables which can be dropped in our next model.
AGE                
INCOME             
EDUCATION          
JOBHome Maker      
JOBStudent         
JOBz_Blue Collar   
CAR_TYPEPanel Truck
RED_CARyes         

From the VIF function we can see 2 variables has VIF > 4. So this multicollinearity issue needs to be fixed. These can be removed from our model in future models. 

From the above Hoslem test we can see the value of p = 0.6951, which is significantly greater than 0.05. Which says our model is not that  good.

From the AUC(Area under the curve) values above of 'r AUC' is  relatively high at .8088. As far as AUC, this model is good.

Considering the hoslem test result this model is creating, this will not be the ideal candidate.

***Base Model Transformation pls Backward Elimination***

2. For this model, we are going to use the Base Model(Transformation) and we are going to remove the variables which has higher p-value(>0.05). 
Also we are going to remove variables which has VIF > 4 from our Model 1.

The following variables have been removed from base_model variables. 



```{r}

train_new = subset(train, select = -c(AGE, INCOME, EDUCATION, JOB, CAR_TYPE))

base_backward_model = glm(TARGET_FLAG ~ . -TARGET_AMT , family = binomial(link = 'logit'), data = train_new)


summary(base_backward_model)

knitr::kable(vif(base_backward_model))

hoslem.test(train$TARGET_FLAG, fitted(base_backward_model))

rocBaseBackwardPlot = roc(base_backward_model$y, fitted(base_backward_model))
AUC = as.numeric(pROC::roc(base_backward_model$y, fitted(base_backward_model))$auc)
AUC



```

***Observations***

From the above model, we see all of our variables has p-values at a significance level lesser than 0.05

From the VIF function we can see 0 variables has VIF > 4. 

From the above Hoslem test we can see the value of p = 0.272, which is little greater than 0.05. Though this value is better compared to our MOdel 1, this is not the best Model for us to pick.

From the AUC(Area under the curve) values above of 'r AUC' has come down a littel compared to Model 1 at .787. 

***Step Model***

3. For our final model, we will use the step function on the base model and transformation variables.


```{r}

base_step_model = step(base_transform_model)
summary(base_step_model)

knitr::kable(vif(base_step_model))

hoslem.test(train$TARGET_FLAG, fitted(base_step_model))
rocBaseStepPlot = roc(base_step_model$y, fitted(base_step_model))
AUC = as.numeric(pROC::roc(base_step_model$y, fitted(base_step_model))$auc)
AUC

```



***Observations***

From the above model, we see 8 variables dropped from 25. 

From the VIF function we can see that the following variables has VIF > 4. JOB is the only variable which has a little higher vIF. 

From the above Hoslem test we can see the value of p = 0.8471, which is more than 0.05. Which says our model is not the best.

From the AUC(Area under the curve) values above of 'r AUC' is  relatively high at .808. This model is good at predicting the response variable.


### Regression Analysis.

***Base plus Transformed Variables.***

For Linear Regression, we will first do as Base Model with transformed variables.

```{r}

regbaseplustransform = lm(TARGET_AMT ~ .-TARGET_FLAG, data = train)
summary(regbaseplustransform)


```

***Observations ****

1. Most of the variables has insignificant p=values. That is values greater than (0.05). 

2. The Mutliple R-Squared and Adjusted R-Squared values are at 69% and 65% respectively. These values are not considered high for model selection.


***BIC Step Model***

Using the transformed values, we are going to do a BIC Forward and Backward selection with missing values imputed. 

```{r}


BICBasePlusTransform = step(regbaseplustransform)
summary(BICBasePlusTransform)


```


```{r}
results = NULL
modellist = list(m1 = regbaseplustransform, m2 = BICBasePlusTransform)

for(i in names(modellist)){
    s = summary(modellist[[i]])
    name = i
    mse <- mean(s$residuals^2)
    r2 <- s$r.squared
    f <- s$fstatistic[1]
    k <- s$fstatistic[2]
    n <- s$fstatistic[3]
    results = rbind(results, data.frame(
        name = name, rsquared = r2, mse = mse, f = f,
        k = k, n = n))
}
rownames(results) = NULL
results

###### Plots ######

plot(fitted(BICBasePlusTransform), resid(BICBasePlusTransform))
abline(h=0)


qqnorm(BICBasePlusTransform$residuals)
qqline(BICBasePlusTransform$residuals)

```


Comparing the base model and BICplusbase model, all the values are near identical from the above table. 
But the QQ plot is not normal as expected with heavier tails.


## 4. MODEL SELECTION:

From our 4 Model, we will first see the ROC and AUC curve.

```{r}

#plot(rocBasePlot, asp=NA, legacy.axes = TRUE, print.auc=TRUE, xlab="FPR")
plot(rocBaseTransformPlot, asp=NA, legacy.axes = TRUE, print.auc=TRUE, xlab="FPR")
plot(rocBaseBackwardPlot, asp=NA, legacy.axes = TRUE, print.auc=TRUE, xlab="FPR")
plot(rocBaseStepPlot, asp=NA, legacy.axes = TRUE, print.auc=TRUE, xlab="FPR")

```

***confusion Matrix.***

```{r}

# Confusion Matrix of Base Transformation Model
baseTransformConfusion = as.factor(as.integer(fitted(base_transform_model ) > .5))
baseTransformCM = confusionMatrix(baseTransformConfusion, as.factor(base_transform_model$y), positive = "1")
caretTransformResults = data.frame(Accurarcy = baseTransformCM$overall[['Accuracy']],
                           ClassErrorRate = 1 - baseTransformCM$overall[['Accuracy']],
                           Precision = baseTransformCM$byClass[['Precision']],
                           Senstivity = baseTransformCM$byClass[['Sensitivity']],
                           Specificity = baseTransformCM$byClass[['Specificity']],
                           F1 = baseTransformCM$byClass[['F1']])

# Confusion Matrix of Base Backward Elimination Model
baseBackwardConfusion = as.factor(as.integer(fitted(base_backward_model ) > .5))
baseBackwardCM = confusionMatrix(baseBackwardConfusion, as.factor(base_backward_model$y), positive = "1")
caretBackwardResults = data.frame(Accurarcy = baseBackwardCM$overall[['Accuracy']],
                           ClassErrorRate = 1 - baseBackwardCM$overall[['Accuracy']],
                           Precision = baseBackwardCM$byClass[['Precision']],
                           Senstivity = baseBackwardCM$byClass[['Sensitivity']],
                           Specificity = baseBackwardCM$byClass[['Specificity']],
                           F1 = baseBackwardCM$byClass[['F1']])

# Confusion Matrix of Base Step Model
baseStepConfusion = as.factor(as.integer(fitted(base_step_model ) > .5))
baseStepCM = confusionMatrix(baseStepConfusion, as.factor(base_step_model$y), positive = "1")
caretStepResults = data.frame(Accurarcy = baseStepCM$overall[['Accuracy']],
                           ClassErrorRate = 1 - baseStepCM$overall[['Accuracy']],
                           Precision = baseStepCM$byClass[['Precision']],
                           Senstivity = baseStepCM$byClass[['Sensitivity']],
                           Specificity = baseStepCM$byClass[['Specificity']],
                           F1 = baseStepCM$byClass[['F1']])
TotalConMatrix = rbind(caretTransformResults, caretBackwardResults, caretStepResults)
knitr::kable(TotalConMatrix)
```

After analyzing all our four models, we will be using the Base_Step_Model for our prediction.

Since all the values are almost identical for all the models we will use Base_Step_Model for our prediction.


## Evalution 

Finally when we when we apply the BIC model to the evalution data, it predicts that there are 205 insurance customers that would have an auto accident and 1936 that would not.

```{r}

eval_results = predict(base_step_model, newdata = test)
table(as.integer(eval_results > .5))

eval_amount = predict(BICBasePlusTransform, test)


```


## References


https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/


